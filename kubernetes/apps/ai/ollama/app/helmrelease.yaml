---
# yaml-language-server: $schema=https://raw.githubusercontent.com/bjw-s-labs/helm-charts/main/charts/other/app-template/schemas/helmrelease-helm-v2.schema.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &app ollama
spec:
  interval: 1h
  chartRef:
    kind: OCIRepository
    name: app-template
  install:
    remediation:
      retries: -1
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  values:
    controllers:
      ollama:
        containers:
          ollama:
            image:
              repository: docker.io/ollama/ollama
              tag: 0.7.1@sha256:de659b95818c5ea17dc287a2e4f147f81e202d84f1dc4ad3f256c53fb81e8dd0
            env:
              TZ: ${CONFIG_TIMEZONE}
              OLLAMA_HOST: 0.0.0.0
              OLLAMA_ORIGINS: "*"
              OLLAMA_MODELS: &modelPath /models
            probes:
              liveness:
                enabled: true
              readiness:
                enabled: true
            resources:
              requests:
                cpu: 500m
                memory: 2Gi
                nvidia.com/gpu: 1 # Request 1 GPU.
              limits:
                memory: 8Gi
                nvidia.com/gpu: 1
            securityContext:
              allowPrivilegeEscalation: false
              readOnlyRootFilesystem: true
              capabilities: { drop: ["ALL"] }
      pull-models:
        type: job
        job:
          ttlSecondsAfterFinished: 300
        containers:
          pull-models:
            image:
              repository: docker.io/curlimages/curl
              tag: 8.13.0@sha256:d43bdb28bae0be0998f3be83199bfb2b81e0a30b034b6d7586ce7e05de34c3fd
            command:
              - /bin/sh
              - -c
              - |
                url="http://ollama.ai.svc.cluster.local:11434"
                models="llama3.2 nomic-embed-text"
                while true; do
                  response=$(curl -s -o /dev/null -w "%{http_code}" "$url")
                  if [ "$response" -eq 200 ]; then
                    echo "Ollama is online."
                    break
                  else
                    echo "Ollama is unreachable. Retrying in 5 seconds..."
                    sleep 5
                  fi
                done
                for model in $models; do
                  echo "Pulling model: $model"
                  curl "$url/api/pull" -d "{\"name\": \"$model\"}"
                done
    defaultPodOptions:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: nvidia.com/gpu.present
                operator: In
                values:
                  - "true"
      nodeSelector:
        nvidia.com/gpu.present: "true"
      priorityClassName: gpu-priority
      runtimeClassName: nvidia
      securityContext:
        runAsNonRoot: true
        runAsUser: &uid 5000
        runAsGroup: *uid
        fsGroup: *uid
        fsGroupChangePolicy: OnRootMismatch
    service:
      app:
        controller: *app
        ports:
          http:
            port: 11434
    ingress:
      app:
        className: internal
        hosts:
          - host: &host "{{ .Release.Name }}.${SECRET_DOMAIN}"
            paths:
              - path: /
                service:
                  identifier: app
                  port: http
        tls:
          - hosts:
              - *host
    persistence:
      models:
        existingClaim: *app
        globalMounts:
          - path: *modelPath
      config:
        type: emptyDir
        globalMounts:
          - path: /.ollama
