---
# yaml-language-server: $schema=https://taskfile.dev/schema.json
version: '3'

tasks:

  browse-pvc:
    desc: Mount a PVC to an temporary container [NS={{.NS}}] [CLAIM=required]
    interactive: true
    cmd: kubectl browse-pvc --namespace {{.NS}} --image docker.io/library/alpine:latest {{.CLAIM}}
    vars:
      NS: '{{.NS | default "default"}}'
    requires:
      vars:
        - CLAIM
    preconditions:
      - kubectl --namespace {{.NS}} get persistentvolumeclaims {{.CLAIM}}
      - kubectl browse-pvc --version
      - which kubectl

  cordon-nodes:
    desc: Cordon all Kubernetes nodes
    cmd: kubectl get nodes -o name | xargs -I {} kubectl cordon {}
    preconditions:
      - which kubectl

  delete-failed-pods:
    desc: Delete pods with a Failed/Pending/Succeeded phase
    cmds:
      - for:
          matrix:
            PHASE:
              - Failed
              - Pending
              - Succeeded
        cmd: kubectl delete pods --all-namespaces --field-selector status.phase={{.ITEM.PHASE}} --ignore-not-found=true
    preconditions:
      - which kubectl

  drain-storage-workloads:
    desc: Cordon nodes and scale down all ceph-block workloads
    deps:
      - cordon-nodes
      - scale-down-ceph-block
    preconditions:
      - which kubectl

  export-cert:
    desc: Export SSL certificate and private key from secret [NS={{.NS}}] [SECRET=required]
    interactive: true
    cmds:
      - kubectl --namespace {{.NS}} get secret {{.SECRET}} -o jsonpath='{.data.tls\.crt}' | base64 --decode > certificate.pem
      - kubectl --namespace {{.NS}} get secret {{.SECRET}} -o jsonpath='{.data.tls\.key}' | base64 --decode > private-key.pem
      - cat private-key.pem certificate.pem > combined.pem
      - openssl x509 -in certificate.pem -text -noout
      - openssl rsa -in private-key.pem -check -noout
    vars:
      NS: '{{.NS | default "default"}}'
    requires:
      vars:
        - SECRET
    preconditions:
      - kubectl --namespace {{.NS}} get secrets {{.SECRET}}
      - which kubectl openssl

  list-resources:
    desc: List common resources in the cluster
    cmds:
      - for: {var: RESOURCE}
        cmd: kubectl get {{.ITEM}} {{.CLI_ARGS | default "-A"}}
    vars:
      RESOURCE: >-
        nodes
        gitrepositories
        kustomizations
        helmrepositories
        ocirepositories
        helmreleases
        certificates
        certificaterequests
        httproutes
        pods
    preconditions:
      - which kubectl

  node-shell:
    desc: Open a shell to a node [NS={{.NS}}] [NODE=required]
    interactive: true
    cmd: kubectl node-shell -n {{.NS}} -x {{.NODE}}
    vars:
      NS: '{{.NS | default "kube-system"}}'
    requires:
      vars:
        - NODE
    preconditions:
      - kubectl get nodes {{.NODE}}
      - kubectl node-shell --version
      - which kubectl

  privileged-pod:
    desc: Run a privileged pod [NODE=required]
    cmd: |
      kubectl run privileged-{{.NODE}} -i --rm --image=null \
        --overrides="$(yq {{.ROOT_DIR}}/.taskfiles/kubernetes/resources/privileged-pod.yaml.j2 -o=json | envsubst)"
    env:
      NODE: '{{.NODE}}'
    preconditions:
      - which kubectl

  restore-storage-workloads:
    desc: Uncordon nodes and scale up all ceph-block workloads
    deps:
      - uncordon-nodes
      - scale-up-ceph-block
    preconditions:
      - which kubectl

  scale-down-ceph-block:
    desc: Scale down all deployments and statefulsets that use PVCs with storageClass=ceph-block
    silent: false
    cmd: |
      # Get all PVCs that use storageClass ceph-block in format "<namespace> <pvc-name>"
      ceph_pvcs=$(
        kubectl get pvc --all-namespaces -o json \
          | jq -r '.items[]
              | select(.spec.storageClassName=="ceph-block")
              | "\(.metadata.namespace) \(.metadata.name)"'
      )

      # Iterate each PVC safely
      while IFS=' ' read -r ns pvc; do
        [ -z "$ns" ] && continue
        echo "Processing PVC $ns/$pvc ..."

        # Scale Deployments referencing this PVC
        deployments=$(
          kubectl get deploy -n "$ns" -o json \
            | jq -r --arg pvc "$pvc" '
                .items[]
                | select(.spec.template.spec.volumes[]?.persistentVolumeClaim.claimName==$pvc)
                | .metadata.name'
        )
        for deploy in $deployments; do
          [ -z "$deploy" ] && continue
          echo "Scaling down Deployment: $ns/$deploy"
          kubectl scale deploy "$deploy" -n "$ns" --replicas=0
        done

        # Scale StatefulSets referencing this PVC
        # For StatefulSets, look at pods that mount the PVC and resolve ownerReferences
        pods=$(kubectl get pods -n "$ns" -o json \
          | jq -r --arg pvc "$pvc" '.items[]
              | select(.spec.volumes[]? | .persistentVolumeClaim.claimName==$pvc)
              | .metadata.name')

        for pod in $pods; do
          owner_kind=$(kubectl get pod -n "$ns" "$pod" -o json | jq -r '.metadata.ownerReferences[0].kind // empty')
          owner_name=$(kubectl get pod -n "$ns" "$pod" -o json | jq -r '.metadata.ownerReferences[0].name // empty')

          if [ "$owner_kind" = "StatefulSet" ] && [ -n "$owner_name" ]; then
            echo "Scaling down StatefulSet: $ns/$owner_name"
            kubectl scale statefulset "$owner_name" -n "$ns" --replicas=0
          fi
        done

      done <<< "$ceph_pvcs"
    preconditions:
      - which jq
      - which kubectl

  scale-up-ceph-block:
    desc: Scale up all deployments and statefulsets that use PVCs with storageClass=ceph-block
    silent: false
    cmd: |
      # Get all PVCs using ceph-block in format "<namespace> <pvc-name>"
      ceph_pvcs=$(
        kubectl get pvc --all-namespaces -o json \
          | jq -r '.items[]
              | select(.spec.storageClassName=="ceph-block")
              | "\(.metadata.namespace) \(.metadata.name)"'
      )

      # Iterate all PVCs
      while IFS=' ' read -r ns pvc; do
        [ -z "$ns" ] && continue
        echo "Processing PVC $ns/$pvc ..."

        # Scale Deployments referencing this PVC
        deployments=$(
          kubectl get deploy -n "$ns" -o json \
            | jq -r --arg pvc "$pvc" '
                .items[]
                | select(.spec.template.spec.volumes[]?.persistentVolumeClaim.claimName==$pvc)
                | .metadata.name'
        )
        for deploy in $deployments; do
          [ -z "$deploy" ] && continue
          echo "Scaling upÂ§  Deployment: $ns/$deploy"
          kubectl scale deploy "$deploy" -n "$ns" --replicas=1
        done

        # Scale StatefulSets referencing this PVC
        # For StatefulSets, look at pods that mount the PVC and resolve ownerReferences
        pods=$(kubectl get pods -n "$ns" -o json \
          | jq -r --arg pvc "$pvc" '.items[]
              | select(.spec.volumes[]? | .persistentVolumeClaim.claimName==$pvc)
              | .metadata.name')

        for pod in $pods; do
          owner_kind=$(kubectl get pod -n "$ns" "$pod" -o json | jq -r '.metadata.ownerReferences[0].kind // empty')
          owner_name=$(kubectl get pod -n "$ns" "$pod" -o json | jq -r '.metadata.ownerReferences[0].name // empty')

          if [ "$owner_kind" = "StatefulSet" ] && [ -n "$owner_name" ]; then
            echo "Scaling down StatefulSet: $ns/$owner_name"
            kubectl scale statefulset "$owner_name" -n "$ns" --replicas=1
          fi
        done

      done <<< "$ceph_pvcs"
    preconditions:
      - which kubectl
      - which jq

  storage-speed-test:
    desc: Run speed test on storage (block or file) [STORAGE=required]
    cmds:
      - task: speed-test-apply-manifest
      - task: speed-test-wait-for-pvc
      - task: speed-test-wait-for-jobs
      - task: speed-test-fetch-logs
      - task: speed-test-cleanup
    requires:
      vars:
        - STORAGE

  speed-test-apply-manifest:
    desc: Apply the Kubernetes manifest for the speed test
    cmds:
      - kubectl apply -f {{.ROOT_DIR}}/.taskfiles/kubernetes/resources/{{.MANIFEST_FILE}}
    vars:
      # Map STORAGE to the corresponding manifest file.
      MANIFEST_FILE:
        sh: |
          if [ "{{.STORAGE}}" = "block" ]; then
            echo "speed-test-block.yaml.j2"
          elif [ "{{.STORAGE}}" = "file" ]; then
            echo "speed-test-file.yaml.j2"
          else
            echo "Unsupported storage type: {{.STORAGE}}" >&2
            exit 1
          fi
    preconditions:
      - which kubectl

  speed-test-wait-for-pvc:
    desc: Wait for the PVC to be fully provisioned
    cmds:
      - kubectl wait --for=jsonpath='{.status.phase}'=Bound pvc/test-claim --timeout=300s
    internal: true

  speed-test-wait-for-jobs:
    desc: Wait for the read and write jobs to complete
    cmds:
      - kubectl wait --for=condition=complete job/write --timeout=600s
      - kubectl wait --for=condition=complete job/read --timeout=600s
    internal: true

  speed-test-fetch-logs:
    desc: Fetch logs from the read and write jobs
    cmds:
      - kubectl logs -l app=speedtest,job=write
      - kubectl logs -l app=speedtest,job=read
    internal: true

  speed-test-cleanup:
    desc: Clean up the Kubernetes resources created for the speed test
    cmds:
      - kubectl delete -f {{.ROOT_DIR}}/.taskfiles/kubernetes/resources/{{.MANIFEST_FILE}}
    vars:
      # Map STORAGE to the corresponding manifest file.
      MANIFEST_FILE:
        sh: |
          if [ "{{.STORAGE}}" = "block" ]; then
            echo "speed-test-block.yaml.j2"
          elif [ "{{.STORAGE}}" = "file" ]; then
            echo "speed-test-file.yaml.j2"
          else
            echo "Unsupported storage type: {{.STORAGE}}" >&2
            exit 1
          fi
    internal: true

  sync-secrets:
    desc: Sync all ExternalSecrets
    cmds:
      - for: {var: SECRETS, split: "\n"}
        cmd: kubectl --namespace {{splitList "," .ITEM | first}} annotate externalsecret {{splitList "," .ITEM | last}} force-sync="{{now | unixEpoch}}" --overwrite
    vars:
      SECRETS:
        sh: kubectl get externalsecret --all-namespaces --no-headers --output=jsonpath='{range .items[*]}{.metadata.namespace},{.metadata.name}{"\n"}{end}'
    preconditions:
      - which kubectl

  uncordon-nodes:
    desc: Uncordon all Kubernetes nodes
    cmd: kubectl get nodes -o name | xargs -r -I {} kubectl uncordon {}
    preconditions:
      - which kubectl
